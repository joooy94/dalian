#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤©ç„¶æ°”é¢„æµ‹APIæœåŠ¡å™¨
æä¾›æµé‡å’Œå‹åŠ›é¢„æµ‹çš„REST APIæ¥å£
"""

import pandas as pd
import numpy as np
from scipy.ndimage import gaussian_filter1d
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import matplotlib.pyplot as plt
import os
import warnings
from datetime import datetime, timedelta
import logging
from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

# å¯¼å…¥APIæ•°æ®è·å–æ¨¡å—
from stl_decomposition.api_data_fetcher import fetch_specific_days

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("API_Gas_Prediction_Server")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="å¤©ç„¶æ°”é¢„æµ‹APIæœåŠ¡",
    description="åŸºäºå†å²3å‘¨åŒæœŸæ•°æ®çš„å¤©ç„¶æ°”æµé‡å’Œå‹åŠ›é¢„æµ‹æœåŠ¡",
    version="1.0.0"
)

# æ•°æ®æ¨¡å‹
class PredictionPoint(BaseModel):
    timestamp: str
    forecast: float

class PredictionResponse(BaseModel):
    success: bool
    prediction_date: str
    metric: str
    data_points: int
    predictions: List[PredictionPoint]

class ErrorResponse(BaseModel):
    error: str
    message: str

class HealthResponse(BaseModel):
    status: str
    service: str
    version: str

class ServiceInfo(BaseModel):
    service: str
    version: str
    endpoints: dict
    description: str

class TestResponse(BaseModel):
    success: bool
    prediction_date: str
    metric: str
    plot_file: str
    message: str

class APIGasPredictor:
    def __init__(self):
        """åˆå§‹åŒ–APIé¢„æµ‹å™¨"""
        self.raw_data = None
        self.smoothed_data_flow = None
        self.smoothed_data_pressure = None
        self.training_data_flow = None
        self.training_data_pressure = None
        
    def load_specific_dates_from_api(self, prediction_date):
        """
        ä»APIåŠ è½½é¢„æµ‹æ‰€éœ€çš„ç‰¹å®šå†å²æ—¥æœŸæ•°æ®
        
        Args:
            prediction_date: é¢„æµ‹æ—¥æœŸï¼ˆå­—ç¬¦ä¸²æˆ–datetimeå¯¹è±¡ï¼‰
        """
        logger.info("æ­£åœ¨ä»APIåŠ è½½é¢„æµ‹æ‰€éœ€çš„å†å²æ•°æ®...")
        
        # ç¡®å®šé¢„æµ‹æ—¥æœŸ
        if isinstance(prediction_date, str):
            prediction_date = pd.to_datetime(prediction_date).date()
        else:
            prediction_date = pd.to_datetime(prediction_date).date()
        
        # è·å–é¢„æµ‹æ—¥æœŸçš„æ˜ŸæœŸå‡ 
        prediction_weekday = pd.to_datetime(prediction_date).weekday()
        weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        prediction_weekday_name = weekday_names[prediction_weekday]
        
        logger.info(f"é¢„æµ‹æ—¥æœŸ: {prediction_date} ({prediction_weekday_name})")
        
        # è®¡ç®—éœ€è¦çš„å†å²æ•°æ®æ—¥æœŸï¼ˆä¸Šå‘¨ã€ä¸Šä¸Šå‘¨ã€ä¸Šä¸Šä¸Šå‘¨çš„åŒä¸€å¤©ï¼‰
        last_week_date = prediction_date - timedelta(days=7)  # ä¸Šå‘¨åŒä¸€å¤©
        two_weeks_ago_date = prediction_date - timedelta(days=14)  # ä¸Šä¸Šå‘¨åŒä¸€å¤©
        three_weeks_ago_date = prediction_date - timedelta(days=21)  # ä¸Šä¸Šä¸Šå‘¨åŒä¸€å¤©
        
        required_dates = [
            three_weeks_ago_date.strftime("%Y-%m-%d"),
            two_weeks_ago_date.strftime("%Y-%m-%d"),
            last_week_date.strftime("%Y-%m-%d")
        ]
        
        logger.info(f"éœ€è¦è·å–çš„å†å²æ•°æ®æ—¥æœŸ:")
        logger.info(f"  ä¸Šä¸Šä¸Šå‘¨åŒä¸€å¤©: {required_dates[0]} ({prediction_weekday_name})")
        logger.info(f"  ä¸Šä¸Šå‘¨åŒä¸€å¤©: {required_dates[1]} ({prediction_weekday_name})")
        logger.info(f"  ä¸Šå‘¨åŒä¸€å¤©: {required_dates[2]} ({prediction_weekday_name})")
        
        # ä»APIè·å–æ•°æ®
        df = fetch_specific_days(required_dates, interval=60000)  # 1åˆ†é’Ÿé—´éš”
        
        if df.empty:
            logger.error("âŒ æ— æ³•ä»APIè·å–æ‰€éœ€çš„å†å²æ•°æ®")
            return False
        
        logger.info(f"âœ… æˆåŠŸè·å– {len(df)} æ¡å†å²æ•°æ®è®°å½•")
        
        # è®¾ç½®æ—¶é—´æˆ³ä¸ºç´¢å¼•
        df.set_index('timestamp', inplace=True)
        df.sort_index(inplace=True)
        
        # ä¿å­˜åŸå§‹æ•°æ®
        self.raw_data = df.copy()
        
        logger.info(f"åŸå§‹æ•°æ®: {len(self.raw_data)} è¡Œ")
        logger.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {self.raw_data.index.min()} è‡³ {self.raw_data.index.max()}")
        
        # ä¿å­˜é¢„æµ‹ä¿¡æ¯
        self.prediction_info = {
            'prediction_date': prediction_date,
            'prediction_weekday_name': prediction_weekday_name,
            'three_weeks_ago_date': three_weeks_ago_date,
            'two_weeks_ago_date': two_weeks_ago_date,
            'last_week_date': last_week_date
        }
        
        return True
        
    def process_data(self, metric_type='flow'):
        """
        å¤„ç†æ•°æ®ï¼Œæ„å»ºè®­ç»ƒæ•°æ®
        
        Args:
            metric_type: 'flow' æˆ– 'pressure'
        """
        logger.info(f"æ­£åœ¨å¤„ç†{metric_type}æ•°æ®...")
        
        # é€‰æ‹©å¯¹åº”çš„æ•°æ®åˆ—
        if metric_type == 'flow':
            target_column = 'æ€»æµé‡'
        else:  # pressure
            target_column = 'å‹åŠ›'
        
        # æ•°æ®å¹³æ»‘ï¼ˆå¯é€‰ï¼‰
        logger.info("æ­£åœ¨å¹³æ»‘æ•°æ®...")
        clean_data = self.raw_data[target_column].dropna()
        smoothed_values = gaussian_filter1d(clean_data.values, sigma=3)  # å‡å°å¹³æ»‘ç¨‹åº¦
        smoothed_data = pd.Series(smoothed_values, index=clean_data.index)
        
        # ä¿å­˜å¹³æ»‘åçš„æ•°æ®
        if metric_type == 'flow':
            self.smoothed_data_flow = smoothed_data
        else:
            self.smoothed_data_pressure = smoothed_data
        
        # æŒ‰æ—¥æœŸåˆ†ç»„æ•°æ®
        df = pd.DataFrame({
            'value': smoothed_data,
            'date': smoothed_data.index.date,
            'hour': smoothed_data.index.hour,
            'minute': smoothed_data.index.minute
        })
        
        # è·å–ä¸‰ä¸ªç‰¹å®šæ—¥æœŸçš„æ•°æ®
        three_weeks_ago_date = self.prediction_info['three_weeks_ago_date']
        two_weeks_ago_date = self.prediction_info['two_weeks_ago_date']
        last_week_date = self.prediction_info['last_week_date']
        
        # åˆ†åˆ«è·å–ä¸‰å¤©çš„æ•°æ®
        three_weeks_data = df[df['date'] == three_weeks_ago_date].copy()
        two_weeks_data = df[df['date'] == two_weeks_ago_date].copy()
        last_week_data = df[df['date'] == last_week_date].copy()
        
        logger.info(f"ä¸Šä¸Šä¸Šå‘¨{metric_type}æ•°æ® ({three_weeks_ago_date}): {len(three_weeks_data)} ä¸ªæ•°æ®ç‚¹")
        logger.info(f"ä¸Šä¸Šå‘¨{metric_type}æ•°æ® ({two_weeks_ago_date}): {len(two_weeks_data)} ä¸ªæ•°æ®ç‚¹")
        logger.info(f"ä¸Šå‘¨{metric_type}æ•°æ® ({last_week_date}): {len(last_week_data)} ä¸ªæ•°æ®ç‚¹")
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if len(three_weeks_data) < 1000 or len(two_weeks_data) < 1000 or len(last_week_data) < 1000:
            logger.error(f"âŒ {metric_type}å†å²æ•°æ®ä¸å¤Ÿå®Œæ•´ï¼Œæ— æ³•è¿›è¡Œå¯é é¢„æµ‹")
            logger.error(f"   éœ€è¦æ¯å¤©è‡³å°‘1000ä¸ªæ•°æ®ç‚¹")
            logger.error(f"   å®é™…: 3å‘¨å‰={len(three_weeks_data)}, 2å‘¨å‰={len(two_weeks_data)}, 1å‘¨å‰={len(last_week_data)}")
            return False
        
        # æŒ‰æ—¶é—´æ’åº
        three_weeks_data = three_weeks_data.sort_values(['hour', 'minute'])
        two_weeks_data = two_weeks_data.sort_values(['hour', 'minute'])
        last_week_data = last_week_data.sort_values(['hour', 'minute'])
        
        # æ„å»ºè®­ç»ƒæ•°æ®ï¼šæŒ‰æ—¶é—´é¡ºåºæ”¾å…¥ä¸‰å‘¨çš„æ•°æ®
        training_data_list = []
        training_data_list.extend(three_weeks_data['value'].values)  # æœ€æ—©çš„æ•°æ®
        training_data_list.extend(two_weeks_data['value'].values)    # ä¸­é—´çš„æ•°æ®
        training_data_list.extend(last_week_data['value'].values)    # æœ€è¿‘çš„æ•°æ®
        
        # åˆ›å»ºè¿ç»­çš„æ—¶é—´ç´¢å¼•ç”¨äºè®­ç»ƒ
        start_time = pd.Timestamp('2024-01-01 00:00:00')  # è™šæ‹Ÿèµ·å§‹æ—¶é—´
        time_index = pd.date_range(start=start_time, periods=len(training_data_list), freq='1min')
        
        training_data = pd.Series(training_data_list, index=time_index)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        if metric_type == 'flow':
            self.training_data_flow = training_data
        else:
            self.training_data_pressure = training_data
        
        logger.info(f"âœ… æ„å»º{metric_type}è®­ç»ƒæ•°æ®å®Œæˆ: {len(training_data)} ä¸ªæ•°æ®ç‚¹")
        logger.info(f"   ä¸‰å‘¨æ•°æ®æ€»é•¿åº¦: {len(three_weeks_data)} + {len(two_weeks_data)} + {len(last_week_data)} = {len(training_data_list)}")
        logger.info(f"è®­ç»ƒæ•°æ®èŒƒå›´: {training_data.min():.2f} ~ {training_data.max():.2f}")
        logger.info(f"è®­ç»ƒæ•°æ®å‡å€¼: {training_data.mean():.2f}")
        
        return True
        
    def predict(self, metric_type='flow', prediction_days=1):
        """
        ä½¿ç”¨ Holt-Winters æ–¹æ³•è¿›è¡Œé¢„æµ‹
        
        Args:
            metric_type: 'flow' æˆ– 'pressure'
            prediction_days: é¢„æµ‹å¤©æ•°ï¼ˆé»˜è®¤1å¤©ï¼‰
        """
        logger.info(f"æ­£åœ¨ä½¿ç”¨ Holt-Winters æ–¹æ³•è¿›è¡Œ{metric_type}é¢„æµ‹...")
        
        try:
            # é€‰æ‹©å¯¹åº”çš„è®­ç»ƒæ•°æ®
            if metric_type == 'flow':
                training_data = self.training_data_flow
            else:
                training_data = self.training_data_pressure
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if training_data.isna().sum() > 0:
                logger.info(f"å‘ç° {training_data.isna().sum()} ä¸ªç¼ºå¤±å€¼ï¼Œè¿›è¡Œæ’å€¼")
                training_data = training_data.interpolate(method='linear')
            
            # è®¾ç½®å­£èŠ‚æ€§å‘¨æœŸä¸º1440åˆ†é’Ÿ(1å¤©)
            seasonal_periods = 1440
            
            if len(training_data) < seasonal_periods * 2:
                seasonal_periods = min(len(training_data) // 4, 720)  # è°ƒæ•´ä¸º4åˆ†ä¹‹ä¸€ï¼Œæœ€å¤§12å°æ—¶å‘¨æœŸ
                seasonal_periods = max(seasonal_periods, 60)  # æœ€å°1å°æ—¶å‘¨æœŸ
                logger.info(f"æ•°æ®è¾ƒå°‘ï¼Œè°ƒæ•´å­£èŠ‚æ€§å‘¨æœŸä¸º: {seasonal_periods} åˆ†é’Ÿ")
            else:
                logger.info(f"ä½¿ç”¨å­£èŠ‚æ€§å‘¨æœŸ: {seasonal_periods} åˆ†é’Ÿ (1å¤©)")
            
            # åˆ›å»º Holt-Winters æ¨¡å‹
            logger.info("æ­£åœ¨åˆ›å»º Holt-Winters æ¨¡å‹...")
            
            model = ExponentialSmoothing(
                training_data,
                trend=None,  # æ— è¶‹åŠ¿ï¼Œæ›´ç¨³å®š
                seasonal='add',  # åŠ æ³•å­£èŠ‚æ€§
                seasonal_periods=seasonal_periods,
                damped_trend=False,
                initialization_method='estimated',
                use_boxcox=False
            )
            
            # æ‹Ÿåˆæ¨¡å‹
            fitted_model = model.fit(optimized=True, remove_bias=False)
            
            # è¿›è¡Œé¢„æµ‹
            prediction_steps = prediction_days * 1440  # è½¬æ¢ä¸ºåˆ†é’Ÿæ•°
            forecast = fitted_model.forecast(steps=prediction_steps)
            
            logger.info(f"åŸå§‹é¢„æµ‹å€¼èŒƒå›´: {forecast.min():.2f} ~ {forecast.max():.2f}")
            logger.info(f"åŸå§‹é¢„æµ‹å€¼å‡å€¼: {forecast.mean():.2f}")
            
            # æ™ºèƒ½ä¿®æ­£é¢„æµ‹å€¼
            data_mean = training_data.mean()
            data_std = training_data.std()
            data_min = training_data.min()
            data_max = training_data.max()
            
            # 1. åå·®æ ¡æ­£
            prediction_bias = forecast.mean() - data_mean
            if abs(prediction_bias) > data_std * 0.12:  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼Œå› ä¸ºæœ‰æ›´å¤šè®­ç»ƒæ•°æ®
                logger.info(f"æ£€æµ‹åˆ°é¢„æµ‹åå·® {prediction_bias:.2f}ï¼Œè¿›è¡Œæ ¡æ­£")
                correction_factor = data_mean / forecast.mean()
                if 0.85 <= correction_factor <= 1.15:  # ç¼©å°æ ¡æ­£èŒƒå›´
                    forecast = forecast * correction_factor
                    logger.info(f"åº”ç”¨ä¹˜æ³•æ ¡æ­£å› å­: {correction_factor:.3f}")
                else:
                    forecast = forecast - prediction_bias
                    logger.info(f"åº”ç”¨åŠ æ³•æ ¡æ­£: {-prediction_bias:.2f}")
            
            # 2. å¤„ç†è´Ÿå€¼
            negative_mask = forecast < 0
            if negative_mask.any():
                logger.info(f"å‘ç° {negative_mask.sum()} ä¸ªè´Ÿå€¼ï¼Œè¿›è¡Œä¿®æ­£")
                replacement_value = max(data_min * 0.95, data_mean * 0.03)
                forecast[negative_mask] = replacement_value
            
            # 3. å¤„ç†æå€¼
            upper_limit = data_max * 1.15  # è¿›ä¸€æ­¥é™ä½ä¸Šé™
            lower_limit = data_min * 0.85  # è¿›ä¸€æ­¥æé«˜ä¸‹é™
            
            extreme_high_mask = forecast > upper_limit
            extreme_low_mask = forecast < lower_limit
            
            if extreme_high_mask.any():
                logger.info(f"å‘ç° {extreme_high_mask.sum()} ä¸ªè¿‡é«˜å€¼ï¼Œé™åˆ¶åœ¨ {upper_limit:.2f}")
                forecast[extreme_high_mask] = upper_limit
            
            if extreme_low_mask.any():
                logger.info(f"å‘ç° {extreme_low_mask.sum()} ä¸ªè¿‡ä½å€¼ï¼Œé™åˆ¶åœ¨ {lower_limit:.2f}")
                forecast[extreme_low_mask] = lower_limit
            
            # 4. è½»åº¦å¹³æ»‘å¤„ç†
            forecast = pd.Series(forecast).rolling(window=3, center=True).mean().fillna(method='bfill').fillna(method='ffill').values
            
            logger.info(f"æœ€ç»ˆé¢„æµ‹å€¼èŒƒå›´: {forecast.min():.2f} ~ {forecast.max():.2f}")
            logger.info(f"æœ€ç»ˆé¢„æµ‹å€¼å‡å€¼: {forecast.mean():.2f}")
            logger.info(f"ä¸è®­ç»ƒæ•°æ®å‡å€¼åå·®: {forecast.mean() - data_mean:.2f}")
            
            # åˆ›å»ºé¢„æµ‹æ—¶é—´ç´¢å¼•
            prediction_date = self.prediction_info['prediction_date']
            prediction_start_time = pd.Timestamp(f"{prediction_date} 00:00:00")
            prediction_index = pd.date_range(
                start=prediction_start_time,
                periods=len(forecast),
                freq='1min'
            )
            
            prediction_series = pd.Series(forecast, index=prediction_index)
            
            # ä¿å­˜é¢„æµ‹ä¿¡æ¯åˆ°ç»“æœä¸­
            prediction_series.prediction_info = self.prediction_info.copy()
            prediction_series.prediction_info['metric_type'] = metric_type
            
            logger.info(f"âœ… {metric_type}é¢„æµ‹å®Œæˆ: {len(prediction_series)} ä¸ªæ•°æ®ç‚¹")
            return prediction_series
            
        except Exception as e:
            logger.error(f"âŒ {metric_type}é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def plot_prediction_results(self, prediction_series, save_path='test_prediction_plot.png'):
        """ç»˜åˆ¶é¢„æµ‹ç»“æœå›¾"""
        logger.info("æ­£åœ¨ç»˜åˆ¶é¢„æµ‹ç»“æœ...")
        
        if not hasattr(prediction_series, 'prediction_info'):
            logger.error("âŒ ç¼ºå°‘é¢„æµ‹ä¿¡æ¯ï¼Œæ— æ³•ç»˜åˆ¶è¯¦ç»†å›¾è¡¨")
            return False
        
        info = prediction_series.prediction_info
        prediction_date = info['prediction_date']
        prediction_weekday_name = info['prediction_weekday_name']
        three_weeks_ago_date = info['three_weeks_ago_date']
        two_weeks_ago_date = info['two_weeks_ago_date']
        last_week_date = info['last_week_date']
        metric_type = info['metric_type']
        
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=(20, 10))
        
        # è·å–å¯¹åº”çš„å¹³æ»‘æ•°æ®
        if metric_type == 'flow':
            smoothed_data = self.smoothed_data_flow
            ylabel = 'ç¬æ—¶æµé‡'
        else:
            smoothed_data = self.smoothed_data_pressure  
            ylabel = 'æ€»å‹åŠ›'
        
        # ä»åŸå§‹æ•°æ®ä¸­è·å–ä¸‰å¤©çš„å†å²æ•°æ®
        df = pd.DataFrame({
            'value': smoothed_data,
            'date': smoothed_data.index.date,
            'hour': smoothed_data.index.hour,
            'minute': smoothed_data.index.minute
        })
        
        three_weeks_data = df[df['date'] == three_weeks_ago_date].copy()
        two_weeks_data = df[df['date'] == two_weeks_ago_date].copy()
        last_week_data = df[df['date'] == last_week_date].copy()
        
        if len(three_weeks_data) > 0 and len(two_weeks_data) > 0 and len(last_week_data) > 0:
            # åˆ›å»ºxè½´æ ‡ç­¾ï¼ˆ24å°æ—¶ï¼‰
            hours = list(range(24))
            
            # è®¡ç®—æ¯å°æ—¶å¹³å‡å€¼
            def get_hourly_averages(data):
                hourly_values = []
                data_sorted = data.sort_values(['hour', 'minute'])
                for hour in range(24):
                    hour_data = data_sorted[data_sorted['hour'] == hour]
                    if len(hour_data) > 0:
                        hourly_values.append(hour_data['value'].mean())
                    else:
                        hourly_values.append(None)
                return hourly_values
            
            three_weeks_values = get_hourly_averages(three_weeks_data)
            two_weeks_values = get_hourly_averages(two_weeks_data)
            last_week_values = get_hourly_averages(last_week_data)
            
            # é¢„æµ‹æ•°æ®æŒ‰å°æ—¶å¹³å‡
            prediction_df = pd.DataFrame({
                'value': prediction_series.values,
                'hour': prediction_series.index.hour
            })
            
            prediction_values = []
            for hour in range(24):
                hour_data = prediction_df[prediction_df['hour'] == hour]
                if len(hour_data) > 0:
                    prediction_values.append(hour_data['value'].mean())
                else:
                    prediction_values.append(None)
            
            # ç»˜åˆ¶å››æ¡çº¿
            ax.plot(hours, three_weeks_values, 
                   color='purple', linewidth=2.5, marker='d', markersize=4,
                   label=f'3å‘¨å‰æ•°æ®: {three_weeks_ago_date} ({prediction_weekday_name})', alpha=0.8)
            
            ax.plot(hours, two_weeks_values, 
                   color='blue', linewidth=2.5, marker='o', markersize=4,
                   label=f'2å‘¨å‰æ•°æ®: {two_weeks_ago_date} ({prediction_weekday_name})', alpha=0.8)
            
            ax.plot(hours, last_week_values, 
                   color='green', linewidth=2.5, marker='s', markersize=4,
                   label=f'1å‘¨å‰æ•°æ®: {last_week_date} ({prediction_weekday_name})', alpha=0.8)
            
            ax.plot(hours, prediction_values, 
                   color='red', linewidth=3, marker='^', markersize=5,
                   label=f'é¢„æµ‹: {prediction_date} ({prediction_weekday_name})', linestyle='--')
            
            # è®¾ç½®å›¾è¡¨
            ax.set_xticks(range(0, 24, 2))
            ax.set_xticklabels([f"{i:02d}:00" for i in range(0, 24, 2)])
            ax.set_title(f'{ylabel}é¢„æµ‹å¯¹æ¯” - {prediction_weekday_name} (APIæ•°æ® - 3å‘¨è®­ç»ƒ)', 
                       fontsize=16, fontweight='bold', pad=20)
            ax.set_xlabel('æ—¶é—´ (å°æ—¶)', fontsize=12)
            ax.set_ylabel(ylabel, fontsize=12)
            ax.grid(True, alpha=0.3)
            ax.legend(loc='upper right', fontsize=11, framealpha=0.9)
            
            # æ·»åŠ è¯´æ˜æ–‡æœ¬
            info_text = f"è®­ç»ƒæ•°æ®:\nâ€¢ 3å‘¨å‰: {three_weeks_ago_date}\nâ€¢ 2å‘¨å‰: {two_weeks_ago_date}\nâ€¢ 1å‘¨å‰: {last_week_date}\nâ€¢ é¢„æµ‹: {prediction_date}"
            ax.text(0.02, 0.98, info_text, transform=ax.transAxes, 
                   verticalalignment='top', fontsize=10,
                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        
        # ä¿å­˜å›¾ç‰‡
        try:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"é¢„æµ‹ç»“æœå›¾å·²ä¿å­˜åˆ°: {os.path.abspath(save_path)}")
            plt.close()
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
            plt.close()
            return False

# å…¨å±€é¢„æµ‹å™¨å®ä¾‹
predictor = APIGasPredictor()

def perform_prediction(target_date, metric_type='flow'):
    """
    æ‰§è¡Œé¢„æµ‹çš„æ ¸å¿ƒå‡½æ•°
    
    Args:
        target_date: é¢„æµ‹æ—¥æœŸ
        metric_type: 'flow' æˆ– 'pressure'
    
    Returns:
        é¢„æµ‹ç»“æœæˆ–None
    """
    try:
        # åŠ è½½æ•°æ®
        if not predictor.load_specific_dates_from_api(target_date):
            return None
        
        # å¤„ç†æ•°æ®
        if not predictor.process_data(metric_type):
            return None
        
        # è¿›è¡Œé¢„æµ‹
        result = predictor.predict(metric_type)
        return result
        
    except Exception as e:
        logger.error(f"é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        return None

@app.get('/predict', response_model=PredictionResponse)
async def predict_flow(date: str = Query(..., description="é¢„æµ‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD")):
    """æµé‡é¢„æµ‹æ¥å£"""
    try:
        # è§£ææ—¥æœŸ
        try:
            target_date = pd.to_datetime(date).date()
        except:
            raise HTTPException(
                status_code=400,
                detail={"error": "æ—¥æœŸæ ¼å¼é”™è¯¯", "message": "è¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼"}
            )
        
        logger.info(f"æ”¶åˆ°æµé‡é¢„æµ‹è¯·æ±‚: {target_date}")
        
        # æ‰§è¡Œé¢„æµ‹
        result = perform_prediction(target_date, 'flow')
        
        if result is None:
            raise HTTPException(
                status_code=500,
                detail={"error": "é¢„æµ‹å¤±è´¥", "message": "æ— æ³•è·å–è¶³å¤Ÿçš„å†å²æ•°æ®æˆ–é¢„æµ‹è¿‡ç¨‹å‡ºé”™"}
            )
        
        # è¿”å›ç»“æœ
        predictions = []
        for timestamp, forecast_value in result.items():
            predictions.append(PredictionPoint(
                timestamp=timestamp.strftime('%Y-%m-%dT%H:%M:%S'),
                forecast=float(forecast_value)
            ))
        
        return PredictionResponse(
            success=True,
            prediction_date=str(target_date),
            metric='ç¬æ—¶æµé‡',
            data_points=len(predictions),
            predictions=predictions
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æµé‡é¢„æµ‹æ¥å£é”™è¯¯: {e}")
        raise HTTPException(
            status_code=500,
            detail={"error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯", "message": str(e)}
        )

@app.get('/predict_pressure', response_model=PredictionResponse)
async def predict_pressure(date: str = Query(..., description="é¢„æµ‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD")):
    """å‹åŠ›é¢„æµ‹æ¥å£"""
    try:
        # è§£ææ—¥æœŸ
        try:
            target_date = pd.to_datetime(date).date()
        except:
            raise HTTPException(
                status_code=400,
                detail={"error": "æ—¥æœŸæ ¼å¼é”™è¯¯", "message": "è¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼"}
            )
        
        logger.info(f"æ”¶åˆ°å‹åŠ›é¢„æµ‹è¯·æ±‚: {target_date}")
        
        # æ‰§è¡Œé¢„æµ‹
        result = perform_prediction(target_date, 'pressure')
        
        if result is None:
            raise HTTPException(
                status_code=500,
                detail={"error": "é¢„æµ‹å¤±è´¥", "message": "æ— æ³•è·å–è¶³å¤Ÿçš„å†å²æ•°æ®æˆ–é¢„æµ‹è¿‡ç¨‹å‡ºé”™"}
            )
        
        # è¿”å›ç»“æœ
        predictions = []
        for timestamp, forecast_value in result.items():
            predictions.append(PredictionPoint(
                timestamp=timestamp.strftime('%Y-%m-%dT%H:%M:%S'),
                forecast=float(forecast_value)
            ))
        
        return PredictionResponse(
            success=True,
            prediction_date=str(target_date),
            metric='æ€»å‹åŠ›',
            data_points=len(predictions),
            predictions=predictions
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å‹åŠ›é¢„æµ‹æ¥å£é”™è¯¯: {e}")
        raise HTTPException(
            status_code=500,
            detail={"error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯", "message": str(e)}
        )

@app.get('/test', response_model=TestResponse)
async def test_prediction(
    date: str = Query(..., description="é¢„æµ‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD"),
    metric: str = Query('flow', description="é¢„æµ‹æŒ‡æ ‡ï¼šflow(æµé‡) æˆ– pressure(å‹åŠ›)")
):
    """æµ‹è¯•é¢„æµ‹æ¥å£ - ç”Ÿæˆé¢„æµ‹ç»“æœå¹¶è¿”å›å¯è§†åŒ–å›¾è¡¨"""
    try:
        # éªŒè¯metricå‚æ•°
        if metric not in ['flow', 'pressure']:
            raise HTTPException(
                status_code=400,
                detail={"error": "å‚æ•°é”™è¯¯", "message": "metricå‚æ•°å¿…é¡»æ˜¯ 'flow' æˆ– 'pressure'"}
            )
        
        # è§£ææ—¥æœŸ
        try:
            target_date = pd.to_datetime(date).date()
        except:
            raise HTTPException(
                status_code=400,
                detail={"error": "æ—¥æœŸæ ¼å¼é”™è¯¯", "message": "è¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼"}
            )
        
        logger.info(f"æ”¶åˆ°æµ‹è¯•é¢„æµ‹è¯·æ±‚: {target_date}, æŒ‡æ ‡: {metric}")
        
        # æ‰§è¡Œé¢„æµ‹
        result = perform_prediction(target_date, metric)
        
        if result is None:
            raise HTTPException(
                status_code=500,
                detail={"error": "é¢„æµ‹å¤±è´¥", "message": "æ— æ³•è·å–è¶³å¤Ÿçš„å†å²æ•°æ®æˆ–é¢„æµ‹è¿‡ç¨‹å‡ºé”™"}
            )
        
        # ç”Ÿæˆå›¾è¡¨
        date_str = str(target_date).replace('-', '')
        plot_filename = f'test_{metric}_prediction_{date_str}.png'
        plot_path = os.path.join('static', plot_filename)
        
        # ç¡®ä¿staticç›®å½•å­˜åœ¨
        os.makedirs('static', exist_ok=True)
        
        success = predictor.plot_prediction_results(result, plot_path)
        
        if not success:
            raise HTTPException(
                status_code=500,
                detail={"error": "å›¾è¡¨ç”Ÿæˆå¤±è´¥", "message": "æ— æ³•ç”Ÿæˆé¢„æµ‹å¯è§†åŒ–å›¾è¡¨"}
            )
        
        metric_name = 'ç¬æ—¶æµé‡' if metric == 'flow' else 'æ€»å‹åŠ›'
        
        return TestResponse(
            success=True,
            prediction_date=str(target_date),
            metric=metric_name,
            plot_file=plot_filename,
            message=f"é¢„æµ‹å®Œæˆï¼Œå…±ç”Ÿæˆ{len(result)}ä¸ªæ•°æ®ç‚¹ã€‚å¯é€šè¿‡ /static/{plot_filename} æŸ¥çœ‹é¢„æµ‹å›¾è¡¨ã€‚"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æµ‹è¯•é¢„æµ‹æ¥å£é”™è¯¯: {e}")
        raise HTTPException(
            status_code=500,
            detail={"error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯", "message": str(e)}
        )

@app.get('/static/{filename}')
async def get_static_file(filename: str):
    """é™æ€æ–‡ä»¶æœåŠ¡ - ç”¨äºè®¿é—®ç”Ÿæˆçš„å›¾ç‰‡"""
    file_path = os.path.join('static', filename)
    if os.path.exists(file_path):
        return FileResponse(file_path)
    else:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶æœªæ‰¾åˆ°")

@app.get('/health', response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return HealthResponse(
        status='healthy',
        service='å¤©ç„¶æ°”é¢„æµ‹API',
        version='1.0.0'
    )

@app.get('/', response_model=ServiceInfo)
async def index():
    """æ ¹è·¯å¾„ä¿¡æ¯"""
    return ServiceInfo(
        service='å¤©ç„¶æ°”é¢„æµ‹APIæœåŠ¡',
        version='1.0.0',
        endpoints={
            'flow_prediction': '/predict?date=YYYY-MM-DD',
            'pressure_prediction': '/predict_pressure?date=YYYY-MM-DD',
            'test_prediction': '/test?date=YYYY-MM-DD&metric=flow/pressure',
            'static_files': '/static/{filename}',
            'health_check': '/health'
        },
        description='åŸºäºå†å²3å‘¨åŒæœŸæ•°æ®çš„å¤©ç„¶æ°”æµé‡å’Œå‹åŠ›é¢„æµ‹æœåŠ¡'
    )

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨å¤©ç„¶æ°”é¢„æµ‹APIæœåŠ¡å™¨ (FastAPI)")
    print("=" * 50)
    print("å¯ç”¨æ¥å£:")
    print("  æµé‡é¢„æµ‹: http://127.0.0.1:58888/predict?date=2025-07-07")
    print("  å‹åŠ›é¢„æµ‹: http://127.0.0.1:58888/predict_pressure?date=2025-07-07")
    print("  æµ‹è¯•é¢„æµ‹: http://127.0.0.1:58888/test?date=2025-07-07&metric=flow")
    print("  å¥åº·æ£€æŸ¥: http://127.0.0.1:58888/health")
    print("  APIæ–‡æ¡£: http://127.0.0.1:58888/docs")
    print("=" * 50)
    
    uvicorn.run(app, host='127.0.0.1', port=58888) 