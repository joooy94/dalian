#!/opt/anaconda3/envs/water/bin/python
# -*- coding: utf-8 -*-
"""
è¯Šæ–­é¢„æµ‹å€¼åå¤§é—®é¢˜
"""

import pandas as pd
import numpy as np
from scipy.ndimage import gaussian_filter1d
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import matplotlib.pyplot as plt

def diagnose_prediction_bias():
    """è¯Šæ–­é¢„æµ‹å€¼åå¤§çš„åŸå› """
    
    print("ğŸ” è¯Šæ–­é¢„æµ‹å€¼åå¤§é—®é¢˜")
    print("=" * 50)
    
    # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
    print("1. æ•°æ®åŠ è½½å’Œå¤„ç†åˆ†æ")
    print("-" * 30)
    
    df = pd.read_excel('test.xlsx', sheet_name='Sheet2')
    df['TagTime'] = pd.to_datetime(df['TagTime'])
    df.set_index('TagTime', inplace=True)
    df.sort_index(inplace=True)
    
    raw_data = df['æ€»ç³»ç»Ÿç¬æ—¶ï¼ˆè®¡ç®—ï¼‰'].copy()
    print(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"  æ•°é‡: {len(raw_data)}")
    print(f"  èŒƒå›´: {raw_data.min():.2f} ~ {raw_data.max():.2f}")
    print(f"  å‡å€¼: {raw_data.mean():.2f}")
    print(f"  ä¸­ä½æ•°: {raw_data.median():.2f}")
    print(f"  æ ‡å‡†å·®: {raw_data.std():.2f}")
    
    # 2. æ£€æŸ¥å¹³æ»‘æ•ˆæœ
    print(f"\n2. å¹³æ»‘å¤„ç†å½±å“åˆ†æ")
    print("-" * 30)
    
    clean_data = raw_data.dropna()
    smoothed_values = gaussian_filter1d(clean_data.values, sigma=5)
    smoothed_data = pd.Series(smoothed_values, index=clean_data.index)
    
    print(f"å¹³æ»‘åæ•°æ®ç»Ÿè®¡:")
    print(f"  èŒƒå›´: {smoothed_data.min():.2f} ~ {smoothed_data.max():.2f}")
    print(f"  å‡å€¼: {smoothed_data.mean():.2f}")
    print(f"  ä¸­ä½æ•°: {smoothed_data.median():.2f}")
    print(f"  æ ‡å‡†å·®: {smoothed_data.std():.2f}")
    
    bias_from_smoothing = smoothed_data.mean() - raw_data.mean()
    print(f"å¹³æ»‘å¼•èµ·çš„å‡å€¼åå·®: {bias_from_smoothing:.2f}")
    
    # 3. æ£€æŸ¥é‡é‡‡æ ·æ•ˆæœ
    print(f"\n3. é‡é‡‡æ ·å¤„ç†å½±å“åˆ†æ")
    print("-" * 30)
    
    resampled_data = smoothed_data.resample('1min').mean()
    resampled_data = resampled_data.interpolate(method='time')
    
    print(f"é‡é‡‡æ ·åæ•°æ®ç»Ÿè®¡:")
    print(f"  èŒƒå›´: {resampled_data.min():.2f} ~ {resampled_data.max():.2f}")
    print(f"  å‡å€¼: {resampled_data.mean():.2f}")
    print(f"  ä¸­ä½æ•°: {resampled_data.median():.2f}")
    print(f"  æ ‡å‡†å·®: {resampled_data.std():.2f}")
    
    bias_from_resampling = resampled_data.mean() - smoothed_data.mean()
    print(f"é‡é‡‡æ ·å¼•èµ·çš„å‡å€¼åå·®: {bias_from_resampling:.2f}")
    
    # 4. æ£€æŸ¥å·¥ä½œæ—¥æ•°æ®é€‰æ‹©åå·®
    print(f"\n4. å·¥ä½œæ—¥æ•°æ®é€‰æ‹©åå·®åˆ†æ")
    print("-" * 30)
    
    df_patterns = pd.DataFrame({
        'value': resampled_data,
        'weekday': resampled_data.index.dayofweek,
        'hour': resampled_data.index.hour,
        'minute': resampled_data.index.minute,
        'date': resampled_data.index.date
    })
    
    weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
    for weekday in range(7):
        weekday_data = df_patterns[df_patterns['weekday'] == weekday]
        if len(weekday_data) > 0:
            print(f"{weekday_names[weekday]}: å‡å€¼={weekday_data['value'].mean():.2f}, "
                  f"æ•°é‡={len(weekday_data)}")
    
    # 5. åˆ†æç‰¹å®šæ—¥æœŸçš„è®­ç»ƒæ•°æ®
    print(f"\n5. è®­ç»ƒæ•°æ®æ„å»ºåˆ†æ")
    print("-" * 30)
    
    # æ¨¡æ‹Ÿé¢„æµ‹7æœˆ7æ—¥ï¼ˆå‘¨ä¸€ï¼‰
    prediction_date = pd.to_datetime('2025-07-07').date()
    last_week_date = prediction_date - pd.Timedelta(days=7)  # 6æœˆ30æ—¥
    two_weeks_ago_date = prediction_date - pd.Timedelta(days=14)  # 6æœˆ23æ—¥
    
    print(f"é¢„æµ‹æ—¥æœŸ: {prediction_date} (å‘¨ä¸€)")
    print(f"éœ€è¦çš„å†å²æ•°æ®: {two_weeks_ago_date}, {last_week_date}")
    
    # è·å–å‘¨ä¸€æ•°æ®
    monday_data = df_patterns[df_patterns['weekday'] == 0]
    daily_data = monday_data.groupby('date')
    
    available_dates = list(daily_data.groups.keys())
    print(f"å¯ç”¨çš„å‘¨ä¸€æ—¥æœŸ: {sorted(available_dates)}")
    
    # æ£€æŸ¥æ‰€éœ€æ—¥æœŸçš„æ•°æ®
    if last_week_date in available_dates and two_weeks_ago_date in available_dates:
        last_week_data = daily_data.get_group(last_week_date)
        two_weeks_ago_data = daily_data.get_group(two_weeks_ago_date)
        
        print(f"\n{two_weeks_ago_date} æ•°æ®ç»Ÿè®¡:")
        print(f"  æ•°é‡: {len(two_weeks_ago_data)}")
        print(f"  èŒƒå›´: {two_weeks_ago_data['value'].min():.2f} ~ {two_weeks_ago_data['value'].max():.2f}")
        print(f"  å‡å€¼: {two_weeks_ago_data['value'].mean():.2f}")
        
        print(f"\n{last_week_date} æ•°æ®ç»Ÿè®¡:")
        print(f"  æ•°é‡: {len(last_week_data)}")
        print(f"  èŒƒå›´: {last_week_data['value'].min():.2f} ~ {last_week_data['value'].max():.2f}")
        print(f"  å‡å€¼: {last_week_data['value'].mean():.2f}")
        
        # æ„å»ºè®­ç»ƒæ•°æ®
        training_data_list = []
        training_data_list.extend(two_weeks_ago_data['value'].values)
        training_data_list.extend(last_week_data['value'].values)
        
        start_time = pd.Timestamp('2024-01-01 00:00:00')
        time_index = pd.date_range(start=start_time, periods=len(training_data_list), freq='1min')
        training_data = pd.Series(training_data_list, index=time_index)
        
        print(f"\nåˆå¹¶è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
        print(f"  æ•°é‡: {len(training_data)}")
        print(f"  èŒƒå›´: {training_data.min():.2f} ~ {training_data.max():.2f}")
        print(f"  å‡å€¼: {training_data.mean():.2f}")
        print(f"  ä¸­ä½æ•°: {training_data.median():.2f}")
        print(f"  æ ‡å‡†å·®: {training_data.std():.2f}")
        
        # ä¸æ€»ä½“æ•°æ®æ¯”è¾ƒ
        overall_bias = training_data.mean() - resampled_data.mean()
        print(f"è®­ç»ƒæ•°æ®ä¸æ€»ä½“æ•°æ®çš„å‡å€¼åå·®: {overall_bias:.2f}")
        
        # 6. åˆ†ææ¨¡å‹é¢„æµ‹åå·®
        print(f"\n6. Holt-Wintersæ¨¡å‹é¢„æµ‹åˆ†æ")
        print("-" * 30)
        
        try:
            seasonal_periods = 1440  # 1å¤©
            if len(training_data) < seasonal_periods * 2:
                seasonal_periods = min(len(training_data) // 3, 360)
                seasonal_periods = max(seasonal_periods, 60)
            
            print(f"ä½¿ç”¨å­£èŠ‚æ€§å‘¨æœŸ: {seasonal_periods}")
            
            # æµ‹è¯•ä¸åŒçš„æ¨¡å‹é…ç½®
            model_configs = [
                {'trend': 'add', 'seasonal': 'add', 'damped_trend': True},
                {'trend': 'add', 'seasonal': 'mul', 'damped_trend': True},
                {'trend': None, 'seasonal': 'add', 'damped_trend': False},
                {'trend': None, 'seasonal': None, 'damped_trend': False}
            ]
            
            for i, config in enumerate(model_configs):
                try:
                    print(f"\næµ‹è¯•é…ç½® {i+1}: {config}")
                    
                    model = ExponentialSmoothing(
                        training_data,
                        trend=config['trend'],
                        seasonal=config['seasonal'],
                        seasonal_periods=seasonal_periods if config['seasonal'] else None,
                        damped_trend=config['damped_trend'],
                        initialization_method='estimated',
                        use_boxcox=False
                    )
                    
                    fitted_model = model.fit(optimized=True, remove_bias=False)
                    forecast = fitted_model.forecast(steps=1440)  # é¢„æµ‹1å¤©
                    
                    print(f"  åŸå§‹é¢„æµ‹èŒƒå›´: {forecast.min():.2f} ~ {forecast.max():.2f}")
                    print(f"  åŸå§‹é¢„æµ‹å‡å€¼: {forecast.mean():.2f}")
                    
                    # åˆ†æé¢„æµ‹åå·®
                    prediction_bias = forecast.mean() - training_data.mean()
                    print(f"  é¢„æµ‹å‡å€¼ä¸è®­ç»ƒæ•°æ®å‡å€¼åå·®: {prediction_bias:.2f}")
                    
                    negative_count = (forecast < 0).sum()
                    extreme_count = (forecast > training_data.max() * 2).sum()
                    print(f"  è´Ÿå€¼æ•°é‡: {negative_count}, æå€¼æ•°é‡: {extreme_count}")
                    
                    if i == 0:  # è¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªé…ç½®
                        # åˆ†ææ¨¡å‹ç»„ä»¶
                        if hasattr(fitted_model, 'level'):
                            print(f"  æ¨¡å‹æ°´å¹³å€¼: {fitted_model.level[-1]:.2f}")
                        if hasattr(fitted_model, 'trend') and fitted_model.trend is not None:
                            print(f"  æ¨¡å‹è¶‹åŠ¿å€¼: {fitted_model.trend[-1]:.2f}")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰è¶‹åŠ¿æ”¾å¤§
                        trend_component = fitted_model.trend if hasattr(fitted_model, 'trend') else None
                        if trend_component is not None and len(trend_component) > 0:
                            recent_trend = trend_component[-100:].mean()  # æœ€è¿‘çš„è¶‹åŠ¿
                            print(f"  æœ€è¿‘è¶‹åŠ¿å‡å€¼: {recent_trend:.2f}")
                            
                            if recent_trend > 1.0:
                                print(f"  âš ï¸  æ£€æµ‹åˆ°æ­£è¶‹åŠ¿ï¼Œå¯èƒ½å¯¼è‡´é¢„æµ‹åå¤§")
                        
                except Exception as e:
                    print(f"  é…ç½® {i+1} å¤±è´¥: {e}")
        
        except Exception as e:
            print(f"æ¨¡å‹åˆ†æå¤±è´¥: {e}")
    
    else:
        print("âš ï¸  æ‰€éœ€çš„å†å²æ—¥æœŸæ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†åˆ†æ")
    
    # 7. ç»™å‡ºä¿®å¤å»ºè®®
    print(f"\n7. ä¿®å¤å»ºè®®")
    print("-" * 30)
    
    total_bias = bias_from_smoothing + bias_from_resampling + (overall_bias if 'overall_bias' in locals() else 0)
    print(f"ç´¯ç§¯åå·®ä¼°è®¡: {total_bias:.2f}")
    
    suggestions = [
        "1. å‡å°‘é«˜æ–¯å¹³æ»‘å¼ºåº¦ (å½“å‰sigma=5ï¼Œå¯å°è¯•sigma=2æˆ–3)",
        "2. æ£€æŸ¥é‡é‡‡æ ·æ–¹æ³•ï¼Œè€ƒè™‘ä½¿ç”¨medianè€Œémean",
        "3. ä½¿ç”¨damped_trend=Trueæ¥æŠ‘åˆ¶è¶‹åŠ¿å¤–æ¨",
        "4. æ·»åŠ é¢„æµ‹åä¿®æ­£ï¼šå°†é¢„æµ‹å‡å€¼è°ƒæ•´åˆ°å†å²å‡å€¼é™„è¿‘",
        "5. è€ƒè™‘ä½¿ç”¨ç§»é™¤è¶‹åŠ¿çš„å­£èŠ‚æ€§æ¨¡å‹",
        "6. å¢åŠ è®­ç»ƒæ•°æ®çš„æ—¶é—´çª—å£ä»¥è·å¾—æ›´ç¨³å®šçš„åŸºçº¿"
    ]
    
    for suggestion in suggestions:
        print(f"  {suggestion}")

if __name__ == "__main__":
    diagnose_prediction_bias() 